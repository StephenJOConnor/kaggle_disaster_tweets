{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"meta_model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMOhl3UAS+THJQfYUDA3FrT"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"p4kdNs3B5Xii","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FJAYgxg6_6lb","colab_type":"code","colab":{}},"source":["df_train=pd.read_csv('../kaggle_disaster_tweets/train.csv')\n","df_test=pd.read_csv('../kaggle_disaster_tweets/test.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RFSo7ZkKBLiL","colab_type":"code","colab":{}},"source":["df_train['text'] = df_train['text'].apply(lambda x: clean_tweet(x))\n","df_test['text'] = df_test['text'].apply(lambda x: clean_tweet(x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-q7cLOPICKVj","colab_type":"code","colab":{}},"source":["#record tagged word\n","df_train['tag_word'] = df_train.apply(lambda x: [w.translate(str.maketrans('', '', string.punctuation)) for w in str(x['text']).lower().split() if '#' in w], axis=1)\n","df_test['tag_word'] = df_test.apply(lambda x: [w.translate(str.maketrans('', '', string.punctuation)) for w in str(x['text']).lower().split() if '#' in w], axis=1)\n","\n","#record mentioned word\n","df_train['@_word'] = df_train.apply(lambda x: [w.translate(str.maketrans('', '', string.punctuation)) for w in str(x['text']).lower().split() if '@' in w], axis=1)\n","df_test['@_word'] = df_test.apply(lambda x: [w.translate(str.maketrans('', '', string.punctuation)) for w in str(x['text']).lower().split() if '@' in w], axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YA8o1qKUVAUV","colab_type":"code","colab":{}},"source":["# hashtag_exists\n","df_train['hashtag'] = df_train['text'].apply(lambda x: 1 if \"#\" in str(x) else 0)\n","df_test['hashtag'] = df_test['text'].apply(lambda x: 1 if \"#\" in str(x) else 0)\n","\n","#delete hashtag\n","df_train['text'] = df_train['text'].apply(lambda x: str(x).replace('#', \"\"))\n","df_test['text'] = df_test['text'].apply(lambda x: str(x).replace('#', \"\"))\n","\n","#mention_exists\n","df_train['mention'] = df_train['text'].apply(lambda x: 1 if \"@\" in str(x) else 0)\n","df_test['mention'] = df_test['text'].apply(lambda x: 1 if \"@\" in str(x) else 0)\n","\n","#delete mention\n","df_train['text'] = df_train['text'].apply(lambda x: str(x).replace('@', \"\"))\n","df_test['text'] = df_test['text'].apply(lambda x: str(x).replace('@', \"\"))\n","\n","#link_exists\n","df_train['link'] = df_train['text'].apply(lambda x: 1 if 'http' in x or 'https' in x else 0)\n","df_test['link'] = df_test['text'].apply(lambda x: 1 if 'http' in x or 'https' in x else 0)\n","\n","# word count\n","df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n","df_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n","\n","# unique_word_count\n","df_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\n","df_test['unique_word_count'] = df_test['text'].apply(lambda x: len(set(str(x).split())))\n","\n","# stop_word_count\n","df_train['stop_word_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n","df_test['stop_word_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n","\n","# url_count\n","df_train['url_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n","df_test['url_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n","\n","#record link\n","df_train['url'] = df_train['text'].apply(lambda x: find_URL(x))\n","df_test['url'] = df_test['text'].apply(lambda x: find_URL(x))\n","\n","#delete link\n","df_train['text'] = df_train['text'].apply(lambda x: remove_URL(x))\n","df_test['text'] = df_test['text'].apply(lambda x: remove_URL(x))\n","\n","# mean_word_length\n","df_train['mean_word_length'] = df_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n","df_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n","\n","# char_count\n","df_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\n","df_test['char_count'] = df_test['text'].apply(lambda x: len(str(x)))\n","\n","# punctuation_count\n","df_train['punctuation_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n","df_test['punctuation_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n","\n","df_train = df_train.fillna('')\n","df_test = df_test.fillna('')\n","\n","#location_given\n","df_train['loc_given'] = df_train['location'].apply(lambda x: 1 if len(str(x).strip())>0 else 0)\n","df_test['loc_given'] = df_test['location'].apply(lambda x: 1 if len(str(x).strip())>0 else 0)\n","\n","#location is in tweet\n","df_train['loc_in_tweet'] = df_train.apply(lambda x: 1 if len([w for w in str(x['location']).lower().split() if w.translate(str.maketrans('', '', string.punctuation)) in str(x['text']).lower()]) > 0 else 0, axis=1)\n","df_test['loc_in_tweet'] = df_test.apply(lambda x: 1 if len([w for w in str(x['location']).lower().split() if w.translate(str.maketrans('', '', string.punctuation)) in str(x['text']).lower()]) > 0 else 0, axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2FlMWwE0j4z","colab_type":"code","colab":{}},"source":["# find emoticons and meanings\n","df_train['emot'] = df_train['text'].apply(lambda x: find_emoticon(x))\n","df_train['emoticon'] = df_train[\"emot\"].apply(lambda x: x.get(\"value\")[0] if x.get(\"value\") else \"\")\n","df_train[\"emotion\"] = df_train[\"emot\"].apply(lambda x: x.get(\"mean\")[0] if x.get(\"mean\") else \"\")\n","\n","df_train = df_train.drop([\"emot\"], axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GaONgli5PB1v","colab_type":"code","colab":{}},"source":["enc = OneHotEncoder(handle_unknown = 'ignore')\n","\n","enc.fit(df_train[['keyword']])\n","\n","df_train = pd.concat([df_train, pd.DataFrame(enc.transform(df_train[['keyword']]).toarray())], axis=1)\n","df_test = pd.concat([df_test, pd.DataFrame(enc.transform(df_test[['keyword']]).toarray())], axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"odH-fLfXSO-y","colab_type":"code","colab":{}},"source":["keyword_dums = []\n","\n","for i in range(0, 222):\n","  keyword_dums.append(i)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xtZhY6LSO--u","colab_type":"text"},"source":["Dummy encoding"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NLlIRE9D8mPu","colab":{}},"source":["enc = OneHotEncoder(handle_unknown = 'ignore')\n","\n","enc.fit(df_train[['keyword']])\n","\n","df_train = pd.concat([df_train, pd.DataFrame(enc.transform(df_train[['keyword']]).toarray())], axis=1)\n","df_test = pd.concat([df_test, pd.DataFrame(enc.transform(df_test[['keyword']]).toarray())], axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MoixOaAW8mP_","colab":{}},"source":["keyword_dums = []\n","\n","for i in range(0, 222):\n","  keyword_dums.append(i)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ftZCuZBkFXZj","colab_type":"text"},"source":["Meta Model"]},{"cell_type":"code","metadata":{"id":"Pylq7vqQIz8w","colab_type":"code","colab":{}},"source":["meta_features = keyword_dums + ['hashtag', 'mention', 'link', 'word_count', 'unique_word_count', 'stop_word_count', 'mean_word_length', 'char_count', 'punctuation_count']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3-1AHqdxFb0E","colab_type":"code","colab":{}},"source":["xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", verbosity=0,\n","                              eval_metric = [\"merror\", \"map\", \"auc\"], random_state=42)\n","\n","parameters = {\"learning_rate\": [0.1, 0.01, 0.001],\n","               \"gamma\" : [0.01, 0.1, 0.3, 0.5, 1, 1.5, 2],\n","               \"max_depth\": [2, 4, 7, 10],\n","               \"colsample_bytree\": [0.3, 0.6, 0.8, 1.0],\n","               \"subsample\": [0.2, 0.4, 0.5, 0.6, 0.7],\n","               \"reg_alpha\": [0, 0.5, 1],\n","               \"reg_lambda\": [1, 1.5, 2, 3, 4.5],\n","               \"min_child_weight\": [1, 3, 5, 7],\n","               \"n_estimators\": [100, 250, 500, 1000]}\n","\n","xgb_rscv = RandomizedSearchCV(xgb_model, param_distributions = parameters, scoring = \"f1_micro\",\n","                             cv = 7, verbose = 3, random_state = 42)\n","\n","model_xgboost = xgb_rscv.fit(df_train_meta[meta_features], df_train_meta['target'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FpzaaP9mPBCT","colab_type":"code","colab":{}},"source":["print(\"Learning Rate: \", model_xgboost.best_estimator_.get_params()[\"learning_rate\"])\n","print(\"Gamma: \", model_xgboost.best_estimator_.get_params()[\"gamma\"])\n","print(\"Max Depth: \", model_xgboost.best_estimator_.get_params()[\"max_depth\"])\n","print(\"Subsample: \", model_xgboost.best_estimator_.get_params()[\"subsample\"])\n","print(\"Max Features at Split: \", model_xgboost.best_estimator_.get_params()[\"colsample_bytree\"])\n","print(\"Alpha: \", model_xgboost.best_estimator_.get_params()[\"reg_alpha\"])\n","print(\"Lamda: \", model_xgboost.best_estimator_.get_params()[\"reg_lambda\"])\n","print(\"Minimum Sum of the Instance Weight Hessian to Make a Child: \",\n","      model_xgboost.best_estimator_.get_params()[\"min_child_weight\"])\n","print(\"Number of Trees: \", model_xgboost.best_estimator_.get_params()[\"n_estimators\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GljPl0jKPUwM","colab_type":"code","colab":{}},"source":["report_best_scores(model_xgboost.cv_results_, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8BbqVDlWT-8P","colab_type":"code","colab":{}},"source":["xgb_model_final = xgb.XGBClassifier(subsample=0.5, reg_lambda=2, reg_alpha=0, n_estimators=1000, min_child_weight=3, max_depth=7, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kp3vJUUrphM7","colab_type":"code","colab":{}},"source":["x = "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5sWLklD3n-9w","colab_type":"code","colab":{}},"source":["X = np.array(df_train_meta[meta_features])\n","y = np.array(df_train_meta['target'])\n","kf = KFold(n_splits=7)\n","kf.get_n_splits(X)\n","\n","k = 1\n","\n","for train_index, val_index in kf.split(X):\n","  \n","  xgb_model_final = xgb.XGBClassifier(subsample=0.5, reg_lambda=2, reg_alpha=0, n_estimators=1000, min_child_weight=3, max_depth=7, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6)\n","  xgb_model_final.fit(X[train_index], np.array(y[train_index]))\n","\n","  if k==1:\n","    meta_preds_train = xgb_model_final.predict_proba(X[val_index])\n","  else:\n","    meta_preds_train = np.concatenate((meta_preds_train, xgb_model_final.predict_proba(X[val_index])), axis=0)\n","\n","  k +=1\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gTj37bB4U5HC","colab_type":"code","colab":{}},"source":["xgb_model_final = xgb.XGBClassifier(subsample=0.5, reg_lambda=2, reg_alpha=0, n_estimators=1000, min_child_weight=3, max_depth=7, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6)\n","xgb_model_final.fit(df_train_meta[meta_features], df_train_meta['target'])\n","\n","meta_preds_test = xgb_model_final.predict_proba(df_test_final[meta_features])\n","meta_preds_results = xgb_model_final.predict_proba(df_test[meta_features])"],"execution_count":0,"outputs":[]}]}